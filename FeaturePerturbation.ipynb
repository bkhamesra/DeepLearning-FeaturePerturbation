{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network - Feature Perturbation Analysis\n",
    "\n",
    "Various problems in industry today relies on deep learning solutions ranging from fraud analytics to computer vision and speech recognition. A common question often of interest is the relation between the initial input and the final output. More specifically, what changes in initial input can alter the final result? While there are several approaches to this, here I present a mathematical approach using perturbation analysis for deep learning.\n",
    "I consider a simple binary classifier with feed forward network and ReLU activations functions but the approach is generalizable to any neural network. This approach allows us to understand the impact of individual features on the n-dimensional probability density surface and can help in better controlling and predicting the effects of each feature. As this method studies the effects of the perturbation of existing surface analytically, we can use this method on pretrained networks which can save significant amount of time. As it is purely analytical, the method is extremely fast and covergent. \n",
    "\n",
    "In this notebook, I will present the mathematical solution, code and a few simple examples to demonstrate the efficiency and limitations of the solution. I will start with a single feature case and extend the solution to multiple features. In separate notebooks, I will demonstrate the applications of this method to some real world cases. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Setup - "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a dataset by randomly drawing sample points from a normal distribution i.e. $X \\sim \\mathcal{N}(0, 1)$. The true mean of the distribution is 0 and variance is 1. We define $\\hat{y}$ as\n",
    "\n",
    "\n",
    "$$ \\hat{y} = \\begin{cases}\n",
    "       1 &\\quad\\text{if} \\,\\, X> 0 \\\\\n",
    "       0 &\\quad\\text{if} \\,\\, X\\leq 0\\\\ \n",
    "     \\end{cases}\n",
    "$$\n",
    "In general the method is extendable to any number of outputs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mathematical Formulation - \n",
    "\n",
    "### Deep Learning Model - \n",
    "Let's start with a trained $L$ layered deep neural network with $n_l$ hidden units where $l = 1....L$. The input vector represented by $X$ has total of n features. The hidden layers use ReLU activation function while the output layer uses sigmoid function. As with a trained network, all the weights and bias terms are known. Then, for given input vector $X$, the forward propagation equations for some layer $l$ can be written as - \n",
    "\n",
    "$$ A^{l} = g^{l}(Z^{l})$$\n",
    "where\n",
    "$$ Z^{l}= W^{l} A^{l-1} + B^{l}$$\n",
    "\n",
    "Here, $A^{l}$ represets the output vector of all nodes in layer l, $g^{l}$ is the activation function for this layer, $W^{l}$ is the weight matrix and $B^{l}$ corresponds to the bias term. For $l=1$, $A^{0} = X$ is the input vector and for $L = 1$, $n_L = 1$ and hence, $A^{l}$ will be a scalar. \n",
    "\n",
    "\n",
    "### Feature Perturbation Analysis\n",
    "Let's assume that output of $X$ was $0$ and we want to find change in input features $\\delta X$ which would change the output to $1$. To achieve this, let's start with perturbing input $X$ (or in our notation $A^{0}$) to $X + \\delta X$ (or $A^{0} + \\delta A^{0}$). Then the new output of the first layer will be given as \n",
    "\n",
    "\\begin{align}\n",
    " \\tilde{Z}^{1} &= W^{1}(A^{0} + \\delta A^{0}) + B^{1} \\\\\n",
    " &= Z^{1} + \\delta Z^{1}\n",
    "\\end{align} \n",
    "where $\\delta Z^{1} = W^{1}\\delta A^{0}$\n",
    "\\begin{align}\n",
    "\\tilde{A}^{1} &= g^{1}(\\tilde{Z}^{1}) \\\\\n",
    "&= g^{1} (Z^{1}) + \\nabla_{Z^{1}} g^{1}(Z^{1}) \\delta Z^{1}\\\\\n",
    "&= g^{1} (Z^{1}) + \\left(\\nabla_{Z^{1}} G^{1}\\right) \\delta Z^{1}\n",
    "\\end{align}\n",
    "\n",
    "where we introduced notation $G^{l} = g^{1}(Z^{1})$ and $\\nabla_{Z^{1}} G^{1}$ is $(n_l, n_l)$ dimensional matrix with gradient of $g^{l}(Z^{l})$ with respect to $Z^{l}$, i.e.\n",
    "\n",
    "\n",
    "$$\\nabla_{Z^{1}} G^{1} = \\left[\\frac{\\partial g^{l}(Z^{l}_i)}{\\partial Z^{l}_j}\\right]_{n_l \\times n_l}$$\n",
    "\n",
    "Here, we used Taylor Expansion in the last step. As the activation functions for hidden layers are ReLU, we can stop the Taylor expansion at first derivatives. Note that ReLU is discontinuous at $Z_i^l=0$ and hence, this method will not work accurately when $Z_i^l$ and $\\tilde{Z}_i^l$ have opposite signs. Generalizing this to layer $l$, we have \n",
    "\n",
    "\\begin{align}\n",
    " \\tilde{Z}^{l}&= Z^{l} + \\delta Z^{l}\\\\\n",
    " \\tilde{A}^{l} &= g^{l} (Z^{l}) + \\left(\\nabla_{Z^{l}} G^{l}\\right) \\delta Z^{l}\n",
    "\\end{align}\n",
    "\n",
    "where $\\delta Z^{l} = W^{l}\\delta A^{l-1}$. Combining these equations , we can obtain the following recursive relation - \n",
    "\\begin{equation}\n",
    "\\delta Z^{l} = W^{l} \\,\\left(\\nabla_{Z^{l-1}} G^{l-1}\\right) \\,\\delta Z^{l-1}\n",
    "\\end{equation}\n",
    "\n",
    "As the activation function for the last layer is Sigmoid, Taylor expansion will not work very efficiently and ignoring higher order derivatives would introduce errors. TO tackle this, let's adopt slightly different strategy. \n",
    "For the output $\\hat{Y} = 1$ for new input $\\tilde{X}$, the probability should be greater than threshold T, i.e. $\\tilde{A}^{L} > T$. As $\\tilde{A}^{L} = \\sigma (\\tilde{Z}^{L})$, we can invert this function to get $\\tilde{Z}^{L}$ in terms of T. This leads to \n",
    "\n",
    "$$ \\delta Z^{L} > \\log{\\left(\\frac{1-T}{T} \\right)} - Z^{L}$$\n",
    "Using the recursive relations derived earlier, we can express $\\delta Z^{L}$ in terms of $\\delta X$ as \n",
    "\n",
    "\\begin{align}\n",
    "\\delta Z^{L} &= W^L\\; \\nabla_{Z^{L-1}} G^{L-1}\\; ..... W^2\\; \\nabla_{Z^{1}} G^{1}\\; W^1 \\delta X\\\\\n",
    "&= \\left(\\prod_{l = 0}^{L-2} W^{L-i}\\nabla_{Z^{L-i-1}} G^{L-i-1}\\;\\right) W^1 \\delta X\\\\\n",
    "&= P^{\\,L1} \\delta X\n",
    "\\end{align}\n",
    "\n",
    "where $ \\delta Z^{1} = W^1 \\delta X$ and $P^{\\,L1}$ is a $(1, N)$ matrix and refers to the coefficient of $\\delta X$ in seond step. Plugging into the inequality for $\\delta Z^{L}$, this yields - \n",
    "$$P^{\\,L1} \\delta X = \\log{\\left(\\frac{1-T}{T} \\right)} - Z^{L}  $$\n",
    "or \n",
    "$$\\sum_{j = 1}^{n_1}P^{\\,L1}_{1j} \\delta X_j = \\log{\\left(\\frac{1-T}{T} \\right)} - Z^{L} $$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single Feature Perturbation - \n",
    "If we perturb a single feature $x_i$, then $\\delta x_i$ can be accurately predicted from above equation. \n",
    "\n",
    "### Multiple Feature Perturbation - \n",
    "If we perturb multiple features s.t. more than 1 element of $\\delta X$ is non-zero, then we require additional system of equations to determine $\\delta X$ correctly. For this, we will minimize the distance of the features from its decision boundary. If we consider N dimensional feature space, where decision boundary segreagates the clusters of point with 0 or 1 class output, then in order to change the output class, one needs to cross the decision bounary. To achieve this in optimal way, we can find the closest point on the decision boundary w.r.t the given point by minimizing this distance. Then, the required change in each feature $\\delta X_i$ will just be the projection of this distance on corresponding feature axis. Let's now see how this can be done -\n",
    "\n",
    "Let's assume $\\delta X$ represents the vector containing smallest change required in each element to change the final output. By construction, the distance of $X$ from the decision boundary will be $ \\sqrt{(\\delta X)^T(\\delta X)}$. However, $\\delta X$ is also constrained by above equation. Thus we have to minimize a function with given constraint which can be achieved with Lagrange Multiplier method. We start with following function - \n",
    "\n",
    "\n",
    "$$ F(\\delta X, \\lambda) := \\delta X^T \\delta X + \\lambda \\left[P^{\\,L1}\\,\\delta X - \\left(\\log{\\left(\\frac{1-T}{T} \\right)} - Z^{L}\\right)\\right] $$\n",
    "\n",
    "And following Lagrange's method, we set the derivatives $dF/d\\lambda = 0$, $dF/dX_i = 0$. Solving these equations yield - \n",
    "\n",
    "\n",
    "$$ \\delta X = \\frac{P^{L1}}{\\sum_{i=1}^N (P^{L1}_i)^2} \\left(\\log{\\left(\\frac{1-T}{T} \\right)} - Z^{L}\\right)$$\n",
    "\n",
    "As a check, we see that above equation recovers the expected result for single feature perturbation. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Things Which needs to be fixed - \n",
    "1. Vanishing determinant problem - if determinant goes to zero, this mthod fails\n",
    "2. Varying delta x - For points near the boundary, delta x is estimated correctly, but for points further away, delta x is off by an order of magnitude. This is related with discontinuity of sigmoid function which can be seen by varying n value. At present we tackle this with iterative procedure, but a better approach is required. \n",
    "3. Here we have assumed the ideal case when neural net accuracy is almost 100%. What needs to be checked is how things change for systems with lower accuracies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import cross_val_score,train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import datetime\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single Feature Perturbation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Case\n",
    "\n",
    "Let's apply this method to a test case in the following steps. \n",
    "* **Sample Data** - Generate a random sample of 10000 points from a normal distribution. \n",
    "* **Neural network** - Train a neural network classifier with multiple layers over these points. Here, I will use MPL Classifier from SKLearn with (2,3,1) layers for simplicity. SKLearn defaults threshold to 0.5 and hence, we choose the default case though the solution is generalizable to any case. \n",
    "* Choose a test dataset and compute the training and test scores. \n",
    "* Compute $\\delta x$ for a few failed case and check the true and predicted value of the new input, $\\tilde{x}$ \n",
    "\n",
    "Note that, even with 10000 points, our mean and variance differ slightly from 0 and 1. As a result, according to neural network, our decision boundary will not be $x>\\mu_p$ where $\\mu_p = 0$ is the population mean, but $x>\\mu_s $ where $\\mu_s$ is the sample mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9000\n",
      "Mean of sample distribution = 0.0116128\n",
      "Variance of sample distribution = 0.995764\n"
     ]
    }
   ],
   "source": [
    "#Change these parameters - \n",
    "\n",
    "hl_sizes = (3,2) #Hidden Layers Sizes\n",
    "verbose=True  #Set this to true to get additional information about weights and biases. \n",
    "boundary = 0\n",
    "#Create the dataset\n",
    "np.random.seed(123)\n",
    "\n",
    "Data = np.random.randn(9000)\n",
    "\n",
    "#Data = np.arange(-100, 100, 0.01) #\n",
    "#Data = Data+ np.random.rand(np.size(Data))\n",
    "\n",
    "import random\n",
    "random.shuffle(Data)\n",
    "\n",
    "Labels = (Data>boundary)\n",
    "print(np.size(Data))\n",
    "\n",
    "Data = np.reshape(Data,(len(Data),1))\n",
    "\n",
    "#Create training and Test datasets\n",
    "X_train, X_test, y_train, y_test = train_test_split( Data, Labels, test_size=0.1, random_state=42)\n",
    "\n",
    "#Check if mean and variance match with normal distribution\n",
    "print(\"Mean of sample distribution = %g\"%np.mean(X_train))\n",
    "print(\"Variance of sample distribution = %g\"%np.var(X_train))\n",
    "\n",
    "#Standardize the data \n",
    "scaler = StandardScaler()\n",
    "X_train_norm =  scaler.fit_transform(X_train)\n",
    "X_test_norm =  scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training score - 1 \n",
      "Test Score = 1\n",
      "\n",
      "Weights - \n",
      " Layer 1 [[-2.79233761  6.89089534  6.51439819]] \n",
      " Layer 2 [[ 1.92354366e-104  2.93620059e+000]\n",
      " [-3.39069959e-002 -4.51509665e+000]\n",
      " [ 7.14101194e-002 -3.51997955e+000]] \n",
      " Layer 3 [[-0.00332347]\n",
      " [-3.32191159]]\n",
      "Bias - \n",
      " Layer 1 [0.77834677 0.37393725 0.31818499] \n",
      " Layer 2: [-0.97270862  1.30859193] \n",
      " Layer 3 [5.20402224] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# SKLearn\n",
    "nn = MLPClassifier(alpha=1e-3, hidden_layer_sizes=hl_sizes, solver='adam', max_iter=1000, activation='relu')  \n",
    "nn.fit(X_train_norm,y_train)\n",
    "print(\"Training score - {:>1.3g} \".format(nn.score(X_train_norm,y_train)))\n",
    "print(\"Test Score = {:>1.3g}\\n\".format(nn.score(X_test_norm,y_test)))\n",
    "\n",
    "\n",
    "if verbose:\n",
    "    print(\"Weights - \\n Layer 1\",nn.coefs_[0], '\\n Layer 2', nn.coefs_[1], '\\n Layer 3', nn.coefs_[2])\n",
    "    print(\"Bias - \\n Layer 1\", nn.intercepts_[0], '\\n Layer 2:',nn.intercepts_[1], '\\n Layer 3',nn.intercepts_[2],'\\n')\n",
    "\n",
    "weight_matrix = nn.coefs_\n",
    "bias = nn.intercepts_\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_Zl(input_array, weight_matrix, bias):\n",
    "    '''Compute the linear output of all the nodes in a layer before applyig the activation function\n",
    "    \n",
    "    Parameters:\n",
    "    input_array (numpy matrix) - input vector (training data/output of previous layer)\n",
    "    weight matrix (numpy matrix) - Weight matrix of trained classifier for that layer\n",
    "    bias (numpy array/matrix) - bias vector for that layer\n",
    "    '''\n",
    "    weight = weight_matrix.T\n",
    "    bias = np.reshape(bias, (np.shape(weight)[0], 1))\n",
    "    Z_l = np.matmul(weight, input_array) + bias\n",
    "    \n",
    "    return Z_l\n",
    "\n",
    "            \n",
    "def Relu(Zi):\n",
    "    return Zi * (Zi > 0)\n",
    "\n",
    "def ModifiedRelu(Zi,n):\n",
    "    \n",
    "    Zi_shape = np.shape(Zi)\n",
    "    Zi_flatten = np.concatenate(Zi)\n",
    "    g = np.zeros(np.size(Zi_flatten))\n",
    "    g[Zi_flatten>0] = Zi_flatten[Zi_flatten>0] + np.log(1 + np.exp(-n*Zi_flatten[Zi_flatten>0]))/n\n",
    "    g[Zi_flatten<=0] = np.log(1 + np.exp(n*Zi_flatten[Zi_flatten<=0]))/n\n",
    "    g = np.reshape(g, Zi_shape)\n",
    "    \n",
    "    return g\n",
    "\n",
    "def ModifiedRelu_Derivative(Zi,n):\n",
    "    return (1+np.exp(-n*Zi))**(-1)\n",
    "\n",
    "def Sigmoid(Zi):\n",
    "    return (1 + np.exp(-Zi))**-1\n",
    "\n",
    "def compute_ZN(weights, intercepts, input_arr, use_modified_relu=False, n=100, verbose=0):\n",
    "    '''Compute output of sigmoid layer. \n",
    "    \n",
    "    Parameters:\n",
    "    weight(numpy matrix) - Weights of trained classifier for all the layers\n",
    "    intercepts (numpy matrix) - Bias terms of trained classifier for all layers\n",
    "    input_arr (numpy matrix) - test data\n",
    "    use_modified_relu (boolean) - Use modified version of relu instead of normal relu\n",
    "    ''' \n",
    "    #Find total number of layers\n",
    "    num_layers = np.shape(weights)[0]\n",
    "    \n",
    "    #Compute the output of each layer\n",
    "    for layer_idx in range(num_layers):\n",
    "            \n",
    "        if layer_idx==0:\n",
    "            A_i_prev = input_arr\n",
    "        else:\n",
    "            if use_modified_relu:\n",
    "                A_i_prev = ModifiedRelu(Z_i,n)\n",
    "            else:\n",
    "                A_i_prev = Relu(Z_i)\n",
    "                \n",
    "        intercept = intercepts[layer_idx]\n",
    "        Z_i = compute_Zl(A_i_prev, weights[layer_idx], intercept)\n",
    "    \n",
    "    Z_N = Z_i\n",
    "    A_N = Sigmoid(Z_i)\n",
    "    \n",
    "    return Z_N, A_N\n",
    "\n",
    "\n",
    "def compute_DeltaZN(weights, intercepts, input_arr, feature_idx, use_modified_relu=False, n=100,verbose=False):\n",
    "    '''Compute the coefficient of delta_x in delta Z^N expression\n",
    "    \n",
    "    Parameters:\n",
    "    weight(numpy matrix) - Weights of trained classifier for all the layers\n",
    "    intercepts (numpy matrix) - Bias terms of trained classifier for all layers\n",
    "    input_arr (numpy matrix) - test data\n",
    "    use_modified_relu (boolean) - Use modified version of relu instead of normal relu\n",
    "    ''' \n",
    "    \n",
    "    #Find total number of layers\n",
    "    num_layers = np.shape(weights)[0]\n",
    "    \n",
    "    #Compute the weight product Π_{l} W_l*Grad([g_{l-1}]) in denominator of (equation 36)\n",
    "    for layer_idx in range(num_layers-1):\n",
    "        if layer_idx==0:\n",
    "            weight_i = weights[layer_idx][feature_idx].T\n",
    "            weight_i = np.reshape(weight_i,(np.size(weight_i),1))\n",
    "            A_i_prev = input_arr\n",
    "        else:\n",
    "            weight_i = weight_product\n",
    "            if use_modified_relu:\n",
    "                A_i_prev = ModifiedRelu(Z_i,n)\n",
    "            else:\n",
    "                A_i_prev = Relu(Z_i)\n",
    "                \n",
    "        intercept = intercepts[layer_idx]\n",
    "        Z_i = compute_Zl(A_i_prev, weights[layer_idx], intercept)\n",
    "\n",
    "        # As dg/dz is a diagonal matrix with elements of 0 or 1, we can reduce it to a column matrix by\n",
    "        # summing over all row components. Then matrix multiplication of (dg/dz)^[l] with delta_z^[l] will\n",
    "        # reduce to elementwise multiplication. \n",
    "        \n",
    "        if use_modified_relu:\n",
    "            dg_dz_i = ModifiedRelu_Derivative(Z_i,n)\n",
    "            dg_dz_i = np.reshape(dg_dz_i,(Z_i.shape[0], Z_i.shape[1]))\n",
    "            \n",
    "        else:\n",
    "            dg_dz_i = np.reshape((Z_i>0),(Z_i.shape[0], Z_i.shape[1]))\n",
    "        weight_ip1 = weights[layer_idx+1].T\n",
    "        #print(dg_dz_i, dg_dz_i*weight_i)\n",
    "        weight_product = np.matmul(weight_ip1, (dg_dz_i*weight_i))\n",
    "        if verbose:\n",
    "            print('Z[{}] = {}'.format(layer_idx, Z_i))\n",
    "            print('a[{}] = {}'.format(layer_idx, Relu(Z_i)))\n",
    "        \n",
    "    zN = compute_Zl(Relu(Z_i), weights[-1], intercepts[-1])\n",
    "    weight_product = np.asscalar(weight_product)\n",
    "    if weight_product==0:\n",
    "        raise ValueError('Denominator is zero')\n",
    "    return weight_product, zN\n",
    "    \n",
    "def compute_SingleFeature_Perturbation( output_prob, threshold, weights, intercepts, input_arr, feature_idx, use_modified_relu=False, n=100, verbose=0):\n",
    "    '''Compute residue delta x which needs to be added to x to change y\n",
    "    \n",
    "    Parameters: \n",
    "    output_prob(float) - output probability of original input\n",
    "    weights (numpy matrix) - Weights of trained classifier for all the layers\n",
    "    intercepts (numpy matrix) - Bias terms of trained classifier for all layers\n",
    "    input_arr (numpy matrix) - test data\n",
    "    feature_idx (int) - perturbed feature index\n",
    "    '''\n",
    "    \n",
    "    orig_ZN, orig_AN = compute_ZN(weights, intercepts, input_arr, use_modified_relu=False, n=100)\n",
    "    if verbose:print('Original Values: X - {}, ZN = {}, AN = {}'.format(input_arr.item(),  orig_ZN, orig_AN))\n",
    "          \n",
    "    if output_prob>threshold and verbose:\n",
    "        print(\"Output probability is already greater than threshold. Hence, no changes required in x.\")\n",
    "        return 0.\n",
    "    \n",
    "    #Check if delta X is small (corresponding output probability should be close to threshold)\n",
    "    elif output_prob<threshold and output_prob>threshold/10:\n",
    "        \n",
    "        if verbose:print(\"Condition: 0.1*Threshold < output_prob < threshold\")\n",
    "        denom , ZN = compute_DeltaZN(weights, intercepts, input_arr, feature_idx,use_modified_relu=use_modified_relu, n=n, verbose=0)\n",
    "        num = -1*np.log((1.-threshold)/threshold) - ZN\n",
    "        \n",
    "        delta_X =  num/denom\n",
    "        delta_ZN = num\n",
    "        new_input = input_arr + delta_X    \n",
    "        new_ZN, new_AN = compute_ZN(weights, intercepts, new_input, use_modified_relu=False, n=100)\n",
    "        iterations = 0\n",
    "        if verbose:print(\"Updated Values - ZN = {}, AN = {}\".format(new_ZN, new_AN))\n",
    "            \n",
    "        \n",
    "    elif output_prob<threshold/10:\n",
    "        if verbose:print(\"Condition: output_prob < 0.1*threshold\")\n",
    "        #new_thresholds = threshold*(np.array(1e-3,1e-2,1e-1,1))\n",
    "        new_input = input_arr\n",
    "        counter = 0\n",
    "        new_AN = output_prob\n",
    "        modified_threshold = threshold\n",
    "        while (new_AN+1e-5<threshold):\n",
    "            \n",
    "            prev_AN = new_AN\n",
    "            prev_input = new_input\n",
    "            T =  min(new_AN*10, threshold)\n",
    "            if verbose: print('Iter = {}, Threshold = {}'.format(counter, T))\n",
    "            #if verbose:print('Threshold - {}, modified_threshold = {},output_prob = {}'.format(T, modified_threshold/10**power, new_AN))\n",
    "            denom , ZN = compute_DeltaZN(weights, intercepts, new_input, feature_idx,use_modified_relu=use_modified_relu, n=n, verbose=0)\n",
    "            num = -1*np.log((1.-T)/T) - ZN\n",
    "            new_delta_X =  num/denom\n",
    "            new_input = new_input + new_delta_X\n",
    "            new_ZN, new_AN = compute_ZN(weights, intercepts, new_input, use_modified_relu=use_modified_relu, n=n)\n",
    "            \n",
    "            if verbose:print(\"Iteration:{} -  Updated ZN = {}, AN = {}\".format(counter+1, new_ZN, new_AN))\n",
    "            \n",
    "            if new_AN > threshold+1e-2 and T>output_prob:\n",
    "                new_AN = prev_AN/2.\n",
    "                new_input = prev_input+1e-8\n",
    "                \n",
    "            elif T<output_prob:\n",
    "                raise ValueError('Current Method based on Taylor Expansion not working. Please try different approach.')\n",
    "                \n",
    "            counter+=1\n",
    "    \n",
    "        delta_X = new_input - input_arr\n",
    "        iterations = counter\n",
    "    return (delta_X, iterations)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computing delta x\n",
    "From above results, we see that training and test scores are 1 which assures us that neural network is working at optimal performance. But note that due to shift in the mean and variance, the decision boundary may be slightly off. Let's now apply our method to a few test cases and see if points with 0 output can be flipped. In the final result, I have shown the original values of x, true and predicted output followed by $\\delta x$ and modified $x$ and corresponding true and predicted output. \n",
    "\n",
    "**Note -** The final output prints the data after normalization by Standard Scaler and hence, to obtain the real values, you would need to apply inverse transform operation of Standard Scaler. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X \t\t Y \t Yhat \t\t δX \t X+δX \t \tY_new \t Yhat_new \t Iterations Used\n",
      "----------------------------------------------------------------------------------------------------\n",
      "-1.389 \t\tFalse \t 0.000 \t\t 1.376 \t -0.012547186 \t False \t 0.504 \t\t 18\n",
      "\n",
      "-0.191 \t\tFalse \t 0.000 \t\t 0.179 \t -0.012547186 \t False \t 0.504 \t\t 3\n",
      "\n",
      "-1.333 \t\tFalse \t 0.000 \t\t 1.321 \t -0.012547186 \t False \t 0.504 \t\t 20\n",
      "\n",
      "-0.856 \t\tFalse \t 0.000 \t\t 0.843 \t -0.012547186 \t False \t 0.504 \t\t 13\n",
      "\n",
      "-0.060 \t\tFalse \t 0.000 \t\t 0.047 \t -0.012547186 \t False \t 0.504 \t\t 5\n",
      "\n",
      "-1.595 \t\tFalse \t 0.000 \t\t 1.583 \t -0.012547186 \t False \t 0.504 \t\t 21\n",
      "\n",
      "-0.457 \t\tFalse \t 0.000 \t\t 0.445 \t -0.012547186 \t False \t 0.504 \t\t 7\n",
      "\n",
      "-0.360 \t\tFalse \t 0.000 \t\t 0.348 \t -0.012547186 \t False \t 0.504 \t\t 5\n",
      "\n",
      "-1.948 \t\tFalse \t 0.000 \t\t 1.935 \t -0.012547186 \t False \t 0.504 \t\t 26\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x432 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#SKlearn\n",
    "\n",
    "print(\"X \\t\\t Y \\t Yhat \\t\\t δX \\t X+δX \\t \\tY_new \\t Yhat_new \\t Iterations Used\")\n",
    "print('-'*100)\n",
    "\n",
    "plt.figure(figsize=(15,6))\n",
    "for i, x in enumerate(np.sort(X_test_norm)):\n",
    "    \n",
    "    x_arr = np.reshape(x, (1, 1))\n",
    "    \n",
    "    yhat  = nn.predict(x_arr)\n",
    "    yhat_prob = nn.predict_proba(x_arr)[0,1]\n",
    "    #print(yhat_prob)\n",
    "    if yhat_prob>0.5: \n",
    "        continue\n",
    "    \n",
    "    try:\n",
    "        delta_x, iterations  = compute_SingleFeature_Perturbation(output_prob=yhat_prob, threshold=0.5, weights=weight_matrix, intercepts=bias,\\\n",
    "                          input_arr=x_arr, feature_idx=0, use_modified_relu=True, n=100, verbose=0)\n",
    "        \n",
    "    except ValueError as ve:\n",
    "        #print(ve)\n",
    "        continue\n",
    "\n",
    "    x_new = x+delta_x\n",
    "    if np.isinf(x_new) or x_new>1e5:\n",
    "        #print('Delta x is infinite')\n",
    "        continue\n",
    "    \n",
    "    # Apply the sample normalization to true mean to account for shift in the mean\n",
    "    normalized_boundary = scaler.transform(np.reshape(boundary, (1,1)))\n",
    "\n",
    "    # Add a small term epsilon so that final output probability is strictly greater than threshold\n",
    "    epsilon = np.amin(np.abs(Data))\n",
    "    yhat_new = nn.predict(x_new+epsilon)\n",
    "    yhat_prob_new = nn.predict_proba(x_new+epsilon)[0,1]\n",
    "    \n",
    "    y_new = x_new+epsilon>normalized_boundary\n",
    "    x_new_arr = np.reshape(x+delta_x, (1, 1))\n",
    "    \n",
    "    if i%50==0:\n",
    "        print(\"{:>1.3f} \\t\\t{} \\t {:>1.3f} \\t\\t {:>1.3f} \\t {:>1.8g} \\t {} \\t {:>1.3f} \\t\\t {}\\n\".format(x.item(), y_test[i],  yhat_prob, delta_x.item(), x_new.item(), y_new.item(), yhat_prob_new.item(), iterations))\n",
    "    plt.subplot(121)\n",
    "    plt.scatter(x.item(), x_new.item())\n",
    "    plt.xlabel('X', fontsize=14)\n",
    "    plt.ylabel('X_new', fontsize=14)\n",
    "    plt.xticks(fontsize=14 )\n",
    "    \n",
    "    plt.subplot(122)\n",
    "    plt.scatter(x.item(), delta_x.item())\n",
    "    plt.xlabel('X', fontsize=14)\n",
    "    plt.ylabel('delta_X', fontsize=14)\n",
    "    plt.xticks(fontsize=12 )\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sparkpy",
   "language": "python",
   "name": "sparkpy"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
